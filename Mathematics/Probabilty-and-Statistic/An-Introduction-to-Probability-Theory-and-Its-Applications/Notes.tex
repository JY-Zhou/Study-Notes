\documentclass{article}
\usepackage{amsmath}
\numberwithin{equation}{subsection}
\author{Jianyu Zhou}
\title{\textbf{Study Notes and Exercise Solutions}\\ \textit{An Introduction of Probability Theory and Its Applications}}
\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\section{The Sample Space}
		\subsection{The Empirical Background}
			\paragraph{Events} Events are the results of experiments or obervations. The events should be distinguished between \textit{compound} (\textit{decomposable}) and \textit{simple} (\textit{indecomposable events}). A compound event is an aggregate of certain simple events.
			\paragraph{Sample points} Sample points are just the simple events. Every indecomposable result of the (idealized) experiment is represented by one, and only one, sample point.
			\paragraph{Sample Space} Sample space is the aggregate of all sample points.
		\subsection{Examples}
			\paragraph{Notes} 
			\begin{enumerate} 
			\item All intuitive background sampling problems are abstractly equivalent to the scheme of \textbf{placing \textit{r} balls into \textit{n} cells}, in the sense that the outcomes differ only in their verbal description. \\(There are some examples listed on P10.)
			\item Here comes a question that how to calculate the size of sampling space (how many sample points in the sample space) when placing \textit{r} balls into \textit{n} cells? This question should be answered under different situations.
				\begin{itemize}
				\item Placing \textit{r} \textbf{distinguishable} balls into \textit{n} \textbf{distinguishable} cells. \\ $$N=n^r$$
				\item Placing \textit{r} \textbf{indistinguishable} balls into \textit{n} \textbf{distinguishable} cells. \\ $$N= {r+n-1 \choose r}$$
				\end{itemize}
			When we facing some particular problems, the model of distinguishable or indistinguishable balls is purely a matter of purpose and convenience.
			\end{enumerate}
		\subsection{The Sample Space and Events} 
			\paragraph{} Sample space is the universe of the sample points. All sample points are indecomposable and cover all the outcomes of an idealized experiment. The event is an aggregate of some sample points (one or the aggregate of all sample points is also an event).
			\paragraph{} New events can be define in terms of two or more given events. Here comes the notation of the formal \textit{algebra of events} (algebra of point sets).
		\subsection{Relations among Events}
			\paragraph{} We use $\Omega$ to denote sample space and capitals to denote events (sets of sample points). The fact that a sample point $x$ is contained in the event $A$ is denoted by $x \in A$. Thus $x \in \Omega$ for every sample point $x$. We write $A=B$ only if the two events consist of exactly the same points.
			\paragraph{Definations} 
				\begin{enumerate}
				\item We shall use the notation $A=0$ to express that the event $A$ contains no sample points. The zero must be interpreted in \textbf{a symbolic sense and not as the numeral}.
				\item The event consisting of all points not contained in the event $A$ will be called the \textit{complementary event }(or \textit{negation}) and will be denoted by $A'$. In particular $\Omega ' = 0$.
				\item With any two events $A$ and $B$ we can associate two new events defined by the conditions ``\textit{both $A$ and $B$ occur}'' and ``\textit{either $A$ or $B$ or both occur}''. These events will be denoted by $AB$ (intersection of $A$ and $B$) and $A\cup B$ (union of $A$ and $B$). If $A$ and $B$ exclude each other, then there are no points common to $A$ and $B$ and the event $AB$ is impossible; analytically this situation is described by the equation $AB=0$ which should be read ``$A$ and $B$ are \textit{mutually exclusive}''.
				\item To every collection $A,B,C,\dots$ of events we define some notions as  follows.
					\begin{itemize}
					\item The aggregate of sample points which belong to all the given sets will be denoted by $ABC\dots$ and called the \textit{intersection} (or \textit{simultaneous realization}).
					\item The aggregate of sample points which belong to at least one of the given sets will be denoted by $A\cup B\cup C\cup \dots$ and called the \textit{union} (or \textit{realization of at least one}).
					\item The events $A,B,C,\dots$ are \textit{mutually exclusive} if no two have a point in common (any two of the events are mutually exclusive), that is, if $AB=0, AC=0, \dots, BC=0, \dots$.
					\end{itemize}
				\item The symbols $A\subset B$ and $B\supset A$ signify that every point of $A$ \textit{is contained in} $B$; they are read, respectively, \textit{$A$ implies $B$} and \textit{$B$ is implied by $A$}. \textbf{If this is the case}, we shall also write $B-A$ instead of $BA'$ to denote the event that \textit{$B$ but not $A$ occurs}. The event $B-A$ contains all those points which are in $B$ but not in $A$. With this notation we can write $A'=\Omega-A$ and $A-A=0$.
					\begin{itemize}
					\item If $A$ and $B$ are mutually exclusive, then the occurrence of $A$ implies the non-occurrence of $B$ and vice versa. Thus $AB=0$ means the same as $A\subset B'$ and as $B\subset A'$.
					\item The event $A-AB$ means the occurrence of $A$ but not of the $A$ and $B$. Thus $A-AB=AB'$.
					\end{itemize}
				\end{enumerate} 
		\subsection{Discrete Sample Spaces}
			\paragraph{Discrete Sample Space} A sample space is called discrete if it contains only finitely many points or infinitely many points which can be arranged into a simple sequence $E_1,E_2\dots$. 
			\paragraph{} The probabilities of events in discrete sample spaces are obtained by mere additions, whereas in other spaces integrations are necessary.
		\subsection{Probabilities in Discrete Sample Spaces: Preparations}
			\paragraph{} The notion of probability is based on an \textit{idealized model}. In a sense, no ideal models exist in reality. But in many applications, it is sufficiently accurate to describe reality and the idealized model can be extremely useful when the experiment is impossible to reconstruct.
			\paragraph{} Here comes some interesting examples. When placing 3 distinguishable balls into 3 cells, there will be 27 distinguishable possibilities. It appears natural to assume that all sample points are \textit{equally probable}, that is, that each sample point has probability $\frac{1}{27}$. If the 3 balls are indistinguishable, there remain 27 different possibilities, even though only ten different forms are distinguishable. This consideration leads us to attribute the following probabilities to this ten points as following Table \eqref{1.6:Table1}.
			\begin{table}
				\begin{center}
					\begin{tabular} {| c | c | c | c |}
						\hline
						Result & Probability & Result & Probability \\ \hline
						$\{***|\:-\:|\:-\:\}$ & $\frac{1}{27}$ & $\{\:*\:|*\:*|\:-\:\}$ & $\frac{1}{9}$ \\ \hline
						$\{\:-\:|***|\:-\:\}$ & $\frac{1}{27}$ & $\{\:*\:|\:-\:|*\:*\}$ & $\frac{1}{9}$ \\ \hline
						$\{\:-\:|\:-\:|***\}$ & $\frac{1}{27}$ & $\{\:-\:|*\:*\:|\:*\:\}$ & $\frac{1}{9}$ \\ \hline
						$\{*\:*|\:*\:|\:-\:\}$ & $\frac{1}{9}$ & $\{\:-\:|\:*\:|*\:*\}$ & $\frac{1}{9}$ \\ \hline
						$\{*\:*|\:-\:|\:*\:\}$ & $\frac{1}{9}$ & $\{\:*\:|\:*\:|\:*\:\}$ & $\frac{2}{9}$\\ \hline			
					\end{tabular}
					\caption{The probability distribution}
					\label{1.6:Table1}
				\end{center}
			\end{table}
			However, in our case with $r=n=3$, Bose and Einstein showed that certain particles attribute equal probability $\frac{1}{10}$ to each of the ten sample points.
			\paragraph{} These examples teach us not to \textbf{rely too much on a priori arguments and to be prepared to accept new and unforeseen schemes}.
		\subsection{The Basic Definitions and Rules}
			\paragraph{Fundamental Convention} Given a discrete sample space $\Omega$ with sample points $E_1, E_2, \dots$, we shall assume that with each point $E_j$ there is associated a number, called the \textit{probability} of $E_j$ and denoted by $P\{E_j\}$. It is to be non-negative and such that
			\begin{equation}
				\label{eq:1.7.1}
				P\{E_1\}+P\{E_2\}+\dots=1.
			\end{equation}
			\paragraph{}Note that the point which probability is zero (which is impossible to occur) is not excluded. In discrete sample spaces, any sample point can be eliminated from the sample space. However, the numerical values of probabilities are not known in advance.
			\paragraph{Definition} The probability $P\{A\}$ of any event $A$ is the sum of the probabilities of all sample point in it.
			\paragraph{} For the sample space, 
			\begin{equation}
				\label{eq:1.7.2}
				P\{\Omega\}=1.
			\end{equation}
			It follows that for any event $A$, 
			\begin{equation}
				\label{eq:1.7.3}
				0\leq P\{A\} \leq 1.
			\end{equation}
			Consider now two arbitrary events $A_1$ and $A_2$. To compute the probability $P\{A_1\cup A_2\}$ that either $A_1$ or $A_2$ or both occur, we have to add the probabilities of all sample points contained either in $A_1$ or in $A_2$, but each point is to be counted only once. Therefore, 
			\begin{equation}
				\label{eq:1.7.4}
				P\{A_1\cup A_2\} \leq P\{A_1\}+P\{A_2\}.
			\end{equation}
			\paragraph{Theorem} For any two events $A_1$ and $A_2$ the probability that either $A_1$ or $A_2$ or both occur is given by 
			\begin{equation}
				\label{eq:1.7.5}
				P\{A_1\cup A_2\} = P\{A_1\}+P\{A_2\}-P\{A_1A_2\}. 
			\end{equation}
			If $A_1A_2 = 0$, that is, if $A_1$ and $A_2$ are mutually exclusive, then the formula reduces to 			
			\begin{equation}
				\label{eq:1.7.6}
				P\{A_1\cup A_2\} = P\{A_1\}+P\{A_2\}.
			\end{equation}			
			The probability $P\{A_1\cup A_2\cup \dots \cup A_n\}$ of the realization of at least one among $n$ events can be computed by a formula analogous to the Formula \eqref{eq:1.7.5} and will be discussed later. For arbitrary events $A_1,A_2,\dots$ the inequality
			\begin{equation}
				\label{eq:1.7.7}
				P\{A_1\cup A_2\cup \dots\} \leq P\{A_1\} + P\{A_2\} + \dots
			\end{equation}
			holds (Occasionally \eqref{eq:1.7.7} referred to as \textit{\textbf{Boole's inequality}}). In the special case where the events $A_1,A_2,\dots$ are mutually exclusive, we have 
			\begin{equation}
				\label{eq:1.7.8}
				P\{A_1\cup A_2\cup \dots\} = P\{A_1\} + P\{A_2\} + \dots
			\end{equation}
			\paragraph{} Finally, the probabilities of all sample points in sample spaces should not be same. Although considering them sharing the same probability will simplify the problem, this assumption will be restricted almost entirely the study of games.
		\subsection{Problems for Solution}
			\paragraph{} These problems are very classic and make me more clear about some notions and enhance what I have learn. Pay more attention to problem 4, 5, 6, 14, 15, 16.
			\begin{enumerate}
			\item (a) $P\{A\} = \frac{3}{5}$, \\ (b) $P\{B\} = \frac{3}{5}\times\frac{2}{4} + \frac{2}{5}\times\frac{3}{4} = \frac{3}{5}$ \\ (c) $P\{C\}=\frac{3\times2}{20} = \frac{3}{10}$  
			\item 6. With the number of points of $S_1, S_2, S_1\cup S_2$ are $12, 12, 18$ respectively, the number of points of $S_1S_2 = 12 + 12 - 18 = 6$. 
			\item $P\{A_1\} = P\{A_2\} = P\{A_3\} = P\{A_4\} = \frac{3\times2\times1}{24} = \frac{1}{4}$, \\ $P\{A_1\cup A_2\}=\frac{3\times2\times1}{24} + \frac{2\times2\times1}{24} = \frac{5}{12}$, (the aggregate of the points which the first place is 1 and the points which the second place is 2 but the first place is not 1) \\ $P\{A_1A_2\} = \frac{2\times1}{24} = \frac{1}{12}$, \\
			so, $P\{A_1\cup A_2\} = P\{A_1\} + P\{A_2\} - P\{A_1A_2\}$ is valid. \\ \dots 
			\item The sample space is all of the outcomes tossing the coin for $N$ times, which contains $2^N$ sample points. Here $N$ is an infinite number. So, let $E_n$ denote the experiment ends after $n (n\geq2)$ tossing, and 
			\begin{equation}
				P\{E_n\} = \frac{2 \times 2^{N-n}}{2^n \times 2^{N-n}} = \frac{1}{2^{n-1}}.
			\end{equation}
			Note that $E_2, E_3,\dots,E_n,\dots$ are mutually exclusive. \\(a) So 
			\begin{equation}
				\begin{aligned}
					P\{A\} &= P\{E_2 \cup E_3 \cup E_4 \cup E_5\} \\
					 &= P\{E_2\} + P\{E_3\} + P\{E_4\} + P\{E_5\} \\ 
					 &= \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \frac{1}{16} = \frac{15}{16}.
				\end{aligned}
			\end{equation}
			\\(b) Similarly, with $k$ is an positive integer,
			\begin{equation}
				\begin{aligned}
					P\{B\} &= P\{E_2 \cup E_4 \cup \dots \cup E_{2k} \cup \dots\} \\
					&= P\{E_2\} + P\{E_4\} + \dots + P\{E_{2k}\} + \dots \\
					&= \frac{1}{2} + \frac{1}{8} + \dots + \frac{1}{2^{2k-1}} + \dots \\
					&=\sum_{k=1}^{\infty}\frac{1}{2^{2k-1}} = \lim_{k \to \infty}\frac{\frac{1}{2}\left(1-\frac{1}{4^k}\right)}{1-\frac{1}{4}} = \frac{2}{3}
				\end{aligned}
			\end{equation}
			\\ \textbf{Notes:} This solution is not totally correct. Although the answers are correct, the sample space is not corresponding to the description. The sample space should contain the two points $HH$ and $TT$ with probability $\frac{1}{4}$; the two points $HTT$ and $THH$ with probability $\frac{1}{8}$; and generally two point with probability $\frac{1}{2^n}$ with $n\geq2$. So, in conclusion, the sample points are the \textbf{outcomes} of the experiment and each point will have it \textbf{own's} probability.
			\item (a) $P\{Unity\} = 2\times(\frac{1}{4} + \frac{1}{8} + \dots + \frac{1}{2^k} + \dots) = 1.$ (Under the description the unity the points (*) is just the sample space.) \\ (b) Let $A,B,C$ denote the events that a, b or c wins respectively. Note that if nobody win the game, the loop shall be either $acbacb\dots$ or $bcabca\dots$. So, a, b winning the game is the either one of the points when there are $3n-1$ or $3n+1$ characters $(n = 1,2,3\dots)$, i.e.
			\begin{equation}
				\begin{aligned}
					P\{A\} = P\{B\} &= \frac{1}{4} + \frac{1}{16} + \dots + \frac{1}{2^{3n-1}} + \frac{1}{2^{3n+1}} + \dots \\ &= \sum_{n=1}^{\infty}\left(\frac{1}{2^{3n-1}}+\frac{1}{2^{3n+1}}\right)\\ &= \lim_{n \to \infty}\frac{\frac{1}{4}\left(1-\frac{1}{8^n}\right)}{1-\frac{1}{8}}+\lim_{n \to \infty}\frac{\frac{1}{16}\left(1-\frac{1}{8^n}\right)}{1-\frac{1}{8}}\\ &= \frac{5}{14}
				\end{aligned}
			\end{equation}
			and c winning the game is the both points when there are $3n$ characters $(n=1,2,3\dots)$, i.e.
			\begin{equation}
				\begin{aligned}
					P\{C\} &= 2 \times \left(\frac{1}{8} + \frac{1}{64} + \dots + \frac{1}{2^{3n}} + \dots \right)\\ &= \sum_{n=1}^{\infty}{\frac{2}{2^{3n}}} \\ &= \lim_{n \to \infty}\frac{\frac{2}{8}\left(1-\frac{1}{8^n}\right)}{1-\frac{1}{8}} = \frac{2}{7}
				\end{aligned}
			\end{equation}
			\\ (c) If no decision is reached at or before the $k$th turn, the game must be end after the $k$th turn. So we can consider the complementary event, i.e.
			\begin{equation}
				\begin{aligned}
					P\{D\} &= 1-P\{D'\}\\ &=1- 2 \times \left(\frac{1}{2^{k+1}} + \frac{1}{2^{k+2}} + \dots \right)\\ &= 1- \sum_{n=k}^{\infty}{\frac{1}{2^{n}}} \\ &= \lim_{n \to \infty}\frac{\frac{1}{2^k}\left(1-\frac{1}{2^n}\right)}{1-\frac{1}{2}} = \frac{1}{2^{k-1}}
				\end{aligned}
			\end{equation}
			\textbf{Note: } This is a good problem, and please review this problem in detail. It tells me that before we calculate the probability of the event, we should \textbf{determine what the sample space is and the probability distribution of the sample points are in advance}. Then we should \textbf{analyze the specific sample points which are included in the event strictly}. Finally, \textbf{take more care about the complementary events and whether the events are mutually exclusive}. This is the correct way to calculate the probability of the event.
			\item \textbf{?}
			\item It is obvious that $A_1A_2A_3 = A_1A_2A_3A_4 \subset A_4$ and $A_1A_2A_3' = A_1A_2A_3'A_4' \subset A_4'$.
			\item (a) It is obvious that $S_1S_2$ and $D_3$ are mutually exclusive, so $S_1S_2D_3 = 0$. \\ (b) Because there are 3 balls, $S_1D_2 = S_1D_2E_3 \subset E_3$ holds. \\ (c) There are only 3 balls, so $E_3 = S_1D_2 \cup S_2D_1$ and $(S_1D_2)(S_2D_1) = 0$. Then $E_3-D_2S_1 \supset S_2D_1$ holds.
			\item (a) $AB$ contains the points $(1,2), (1,4), (1,6), (2,1), (4,1), (6,1)$, so $P\{AB\} = \frac{1}{6}$. \\ (b) $P\{A \cup B\} = P\{A\} + P\{B\} - P\{AB\} = \frac{1}{2} + \frac{11}{36} - \frac{1}{6} = \frac{23}{36}$. \\ (c) $P\{AB'\} = \frac{12}{36} = \frac{1}{3}$.
			\item (a) The husband and wife are both older than 40 and the husband is older than wife. \\ (b) The husband is older than 40 but younger than wife. \\ (c) The husband and wife are both older than 40 and the husband is younger than wife.
			\item $AC'$ indicates that the husband is older than 40 but the wife is younger than 40, so the husband must be older than wife and $B$ holds.
			\item (a) 0. (b) 0. (c) 4. (d) 2. (e) 1. (f) 1. (g) 0.
			\item The verifications are easy to think up considering the description of the problem.
			\item The solution can be got by drawing some figures easily. Here I copy these formulas to enhance the impression. \\ (a) $(A\cup B)' = A'B'$ \\ (b) $(A\cup B)-B = A-AB = AB'$ \\ (c) $AA = A\cup A = A$ \\ (d) $(A-AB)\cup B = A \cup B$ \\ (e) $(A\cup B) - AB = AB' \cup A'B$ \\ (f) $A'\cup B' = (AB)'$ \\ (g) $(A\cup B)C = AC\cup BC$
			\item (a) $(A\cup B)(A\cup B') =(A\cup B)A \cup (A\cup B)B'=AA\cup BA\cup AB' \cup BB' = A \cup AB' = A$ \\ (b) $(A\cup B)(A'\cup B)(A\cup B') = B(A\cup B') = BA \cup BB' = AB$ \\ (c) $(A\cup B)(B\cup C) = AB \cup BB\cup AC\cup BC = B\cup AC\cup BC = B \cup AC$ \\ \textbf{Notes: } $AB \subset A $, $AB \subset B$. Do not confuse.
			\item True: (c), (d), (e), (f), (h), (i), (k), (l)\\ False: \\ (a) The equation will be correct only if $C\subset B$. \\ (b) When $AB \not= 0, AC = 0, BC = 0$ the equation is invalid. \\  (g) If $AB\not=0$, the equation is invalid. It should be $(A\cup B)-A = A'B$. \\ (j) Totally wrong. (k) is the correct version. \\ \textbf{Notes: } In this book, $B-C$ is valid only when $C \subset B$.
			\item (a) $AB'C'$ \\ (b) $ABC'$ \\ (c) $ABC$ \\ (d) $A\cup B\cup C$ \\ (e) $AB\cup AC\cup BC$ \\ (f) $AB'C'\cup A'BC'\cup A'B'C$ \\ (g) $ABC'\cup AB'C\cup A'BC$ \\ (h) $A'B'C'$ \\ (i) $(ABC)'$
			\item $A\cup B\cup C = A\cup(B-AB)\cup(C-AC\cup BC) = A\cup A'B\cup A'B'C$
			\item $A, B-AB$ and $C-AC\cup BC$ are mutually exclusive, so 
			\begin{equation}
				\begin{aligned}
					P\{A\cup B\cup C\} &= P\{A\} + P\{A'B\} + P\{A'B'C\} \\ &= P\{A\} + P\{B\} - P\{AB\} + P\{C\} - P\{AC\cup BC\} \\ &= P\{A\} + P\{B\} - P\{AB\} + P\{C\} \\ & - (P\{AC\} + P\{BC\} - P\{ABC\}) \\ &= P\{A\} + P\{B\} + P\{C\} - P\{AB\} - P\{AC\} - P\{BC\} \\& + P\{ABC\}
				\end{aligned}
			\end{equation}
			\end{enumerate}
			
	\newpage
	\section{Elements of Combinatorial Analysis}
		\paragraph{} The following chapters (chapter 2, 3, 4) are not in conjunction with the chapter 1. These are some complementary knowledge. Chapter 5 will take up the theoretical thread of chapter 1.
		\subsection{Preliminary}
			\paragraph{Pairs.} With $m$ elements $a_1,\dots,a_m$ and $n$ elements $b_1,\dots,b_n$, it is possible to form $mn$ pairs $(a_j,b_k)$ containing one element from each group.
			\paragraph{Multiplets.} Given $n_1$ elements $a_1,\dots,a_{n_1}$ and $n_2$ elements $b_1,\dots,b_{n_2}$, etc., up to $n_r$ elements $x_1,\dots,x_{n_r}$; it is possible to form $n_1n_2\dots n_r$ ordered $r$-tuples $(a_{j_1}, b_{j_2}, \dots, x_{j_r})$ containing one element of each kind.
			\paragraph{} Many applications are based on the following reformulation of the last theorem: $r$ successive selections (decisions) with exactly $n_k$ choices possible at the $k$th step can produce a total of $n_1n_2\dots n_r$ different results.
		\subsection{Ordered Samples}
			\paragraph{Ordered Sample. } Consider the population (set) of $n$ elements $a_1, a_2,\dots, a_n$. Any ordered arrangement $a_{j_1}, a_{j_2}, \dots, a_{j_r}$ of $r$ symbols is call an \textit{ordered sample of size $r$} drawn from the population.
			\paragraph{} Here two procedures are possible, \textit{sampling with replacement} and \textit{sampling without replacement}.
			\begin{itemize}
			\item \textbf{Sampling with replacement}. Here a selection is made from the entire population, so that the same element can be drawn more than once. The samples are then arrangements in which repetitions are permitted. \\ In this case, each of the $r$ element can be chosen in $n$ ways. The number of possible samples is therefore $n^r$, as can be seen with theorem with $n_1 = n_2 = \dots = n$.
			\item \textbf{Sampling without replacement}. Here an element once chosen is removed from the population, so that the sample becomes an arrangement without repetitions. \\ In this case, $r$ cannot exceed $n$. We have $n$ possible choices for the first element, but only $n-1$ for the second, $n-2$ for the third, etc., and so there are $n(n-1)\dots (n-r+1)$ choices in all. This appears so often and it is convenient to introduce the notation 
			\begin{equation}
				\label{eq:2.2.1}
				(n)_r = n(n-1)\dots (n-r+1).
			\end{equation}
			Clearly $(n)_r = 0$ for integers $r, n$ such that $r > n$.
			\end{itemize}			
			\paragraph{Theorem. } For a population of $n$ elements and a prescribed sample size $r$, there exist $n^r$ different samples with replacement and $(n)_r$ samples without replacement.			
			\paragraph{} We note the special case where $r=n$. $n$ elements $a_1,\dots,a_n$ can be ordered in $(n)_n = n(n-1)\dots2\times1$ different ways. Instead of $(n)_n$ we write $n!$ which is the more usual notion.
			\paragraph{Corollary. } The number of different ordering of $n$ elements is
			\begin{equation}
				n! = n(n-1)\dots 2\times 1.
			\end{equation}
			\paragraph{} Whenever we speak of \textit{random samples} of fixed size $r$, the adjective \textit{random} is to imply that all possible sample points have the \textit{same probability}, namely, $n^{-r}$ in sampling with replacement and $\frac{1}{(n)_r}$ without replacement. If $n \gg r$, the ratio $\frac{(n)_r}{n^r}$ is near unity. This leads us to expect that, for large populations and relatively small samples, two ways of sampling are practically equivalent.
			\paragraph{} In sampling without replacement the probability for any fixed element of the population to be include in a random sample of size $r$ is
			\begin{equation}
				1-\frac{(n-1)_r}{(n)_r} = 1-\frac{n-r}{n} = \frac{r}{n}
			\end{equation}
			The complementary event is the probability that the fixed element is not chosen. This equation also indicates that no matter when the element will be chosen, the probability for any fixed element being chosen is always $\frac{r}{n}$.
			\paragraph{} In sampling with replacement the probability that an element be included at least once is
			\begin{equation}
				1 - {\left( 1 - \frac{1}{n} \right)}^r
			\end{equation}
		\subsection{Examples}
			\paragraph{} This section concerns the probability of the event that in the sample no element appears twice from a random sample of size $r$ with replacement is taken from a population of $n$ elements. According to the previous section, there are $n^r$ different samples in all, of which $(n)_r$ satisfy the stipulated condition. Assuming all arrangements have equal probability, the probability is 
			\begin{equation}
				\label{eq:2.3.1}
				p = \frac{(n)_r}{n^r}
			\end{equation}
			\paragraph{} Since sometimes it's hard to calculate the exact value of formula \eqref{eq:2.3.1}. Here comes two good numerical approximation to $p$. Here we deduce \eqref{eq:2.3.1} into following form.
			\begin{equation}
				\label{eq:2.3.2}
				p = \frac{(n)_r}{n^r} = \left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)\dots\left(1-\frac{r-1}{n}\right)
			\end{equation} 
			When $r$ is small, we can neglect all cross products and have in approximation
			\begin{equation}
				\label{eq:2.3.3}
				p \approx 1-\frac{1+2+\dots+(r-1)}{n} = 1-\frac{r(r-1)}{2n}
			\end{equation}
			For larger $r$ (still stipulate $r\ll n$) we obtain a much better approximation by passing to logarithms. For small positive $x$ we have $\ln(1-x)\approx-x$, and thus from \eqref{eq:2.3.2}
			\begin{equation}
				\label{eq:2.3.4}
				\begin{aligned}
					\ln p &= \ln \left(1-\frac{1}{n}\right)+\ln \left(1-\frac{2}{n}\right)+\dots+\ln \left(1-\frac{r-1}{n}\right)\\ &\approx-\frac{1+2+\dots+(r-1)}{n} = -\frac{r(r-1)}{2n}
				\end{aligned}			
			\end{equation}
		\subsection{Subpopulations and partitions}
			\paragraph{} As before, we use the term population of size $n$ to denote an aggregate of $n$ elements without regard to their order. Now consider a subpopulation of size $r$ of a given population consisting of $n$ elements. Since $r$ elements can be numbered in $r!$ different ways, it follows that there are exactly $r!$ times as many samples as there are subpopulations of size $r$. The number of subpopulations of size $r$ is therefore given by $\frac{(n)_r}{r!}$. Expressions of this kind are known as \textit{\textbf{binomial coefficients}}, and the standard notatoin for them is
			\begin{equation}
				\label{eq:2.4.1}
				{n \choose r} = \frac{(n)_r}{r!} = \frac{n(n-1)\dots(n-r+1)}{r(r-1)\dots 2\cdot 1}
			\end{equation}
			\paragraph{Theorem} A population of $n$ elements possesses $n \choose r$ different subpopulations of size $r \leq n$.
			\paragraph{} When we choose a subset of $r$ elements, another subset of size $n-r$ is also determined. It follows that there are as many as subpopulations of size $r$ as there are subpopulations of size $n-r$, and hence for $1 \leq r \leq n$ we must have
			\begin{equation}
				\label{eq:2.4.2}
				{n \choose r} = {n \choose n-r}
			\end{equation}
			To prove equation \eqref{eq:2.4.2} directly we observe that an alternative way of writing the binomial coefficient \eqref{eq:2.4.1} is
			\begin{equation}
				\label{eq:2.4.3}
				{n \choose r} = \frac{n!}{r!(n-r)!}
			\end{equation}
			Note that the left side in equation \eqref{eq:2.4.2} is not defined for $r=0$, but the right side is. In order to make equation \eqref{eq:2.4.2} valid for all integers $r$ such that $0\leq r\leq n$, we now define
			\begin{equation}
				\label{eq:2.4.4}
				{n \choose 0} = 1,\indent 0! = 1,\indent(n)_0 = 1.
			\end{equation}
			\paragraph{}\textbf{Stirling's formula} can help us calculate the approximation of binomial coefficient, which will be disscussed in section 9.
			\paragraph{} The distinction between distinguishable and indistinguishable elements is similar to the relationship between a subpopulation and the corresponding ordered samples. Deleting the subscripts in an arragement (or grouping) of $r$ elements $a_1,\dots,a_r$ yields an arrangement of $r$ indistinguishable letters. Different from considering a subpopulation as to \textbf{choose $r$ elements from $n$}, the alternative way to understand this is to \textbf{eliminate the repetitive counting of the order information from ordered samples (divide $(n)_r$ by $r!$)}. The latter idea helps us to solve more problems like the examples on P36 (plz review in detail) and lead us to the theorem of partition.
			\paragraph{Theorem} Let $r_1,\cdots,r_k$ be integers such that
			\begin{equation}
				r_1+r_2+\cdots+r_k=n,\indent r_i\geq 0
			\end{equation}
			The number of ways in which a population of $n$ elements can be divided into $k$ ordered parts (partitioned into $k$ subpopulations) of which the first contains $r_1$ elements, the second $r_2$ elements, etc., is
			\begin{equation}
				\label{eq:2.4.6}
				\frac{n!}{r_1!r_2!\cdots r_k!}
			\end{equation}
			This is called \textit{\textbf{multinomial coefficients}}.
			\paragraph{} Note that the order of the subpopulations is essential in the sense that ($r_1=2,r_2=3$) and ($r_1=3,r_2=2$) represent different partitions; however, no attention is paid to the order within the groups. It is permitted that $r_i=0$ because $0!=1$ makes it no way affect formula \eqref{eq:2.4.6}.
			\paragraph{Proof} A repeat use of \eqref{eq:2.4.3} will show that \eqref{eq:2.4.6} may be rewritten in the form
			\begin{equation}
				{n \choose r_1}{n-r_1 \choose r_2}{n-r_1-r_2 \choose r_3}\cdots{n-r_1-\cdots-r_{k-2} \choose r_{k-1}}
			\end{equation}
			On the other hand, in order to effect the desired partition, we have first to select $r_1$ elements out of the given $n$; of the remaining $n-r_1$ elements we select a second group of size $r_2$, etc. After forming the ($k-1$)st group there remain $n-r_1-r_2-\cdots-r_{k-1}=r_k$ elements and form the last group.
		\subsection{Application to Occupancy Problems} 
			\paragraph{} The examples of chapter 1.2, indicate the wide applicability of the model of placing randomly $r$ balls into $n$ cells. In many situations it is necessary to treat the balls as indistinguishable. Such an event is completely described by its \textit{occupancy numbers} $r_1,r_2,\dots,r_n$, where $r_k$ stands for the number of balls in the $k$th cell. Every $n$-tuple of integers satisfying
			\begin{equation}
				\label{eq:2.5.1}
				r_1+r_2+\cdots+r_n=r,\indent r_k\geq 0
			\end{equation}
			describes a possible configuraiton of occupancy numbers.\textit{ With indistinguishable balls two distributions are distinguishable only if the corresponding n-tuples ($r_1, r_2,\dots, r_n$) are not identical}. We now prove that:
			\begin{enumerate}
			\item[(\textit{i})] The number of distinguishable distributions is
			\begin{equation}
				A_{r,n} = {n+r-1\choose r} = {n+r-1\choose n-1}
			\end{equation}
			\item[(\textit{ii})] The number of distinguishable distributions in which no cell remains empty is 
			\begin{equation}
				\label{eq:2.5.3}
				{r-1\choose n-1}
			\end{equation}
	
			\end{enumerate}
			\paragraph{Proof} ($i$) The problem can be transformed into using $n-1$ bars to separate $r$ balls. It indicates that the $n-1$ bars and $r$ balls can appear in an arbitrary order. In this way it becomes apparent that the number of distinguishable distributions equals the number of ways of selecting $r$ places out of $n+r-1$, namely $A_{r,n}$. ($ii$) The condition that no cell be empty imposes the restriction that no two bars be adjacent. The $r$ balls will leave $r-1$ spaces of which $n-1$ are to be occupied by bars: thus we have $r-1 \choose n-1$ and the assertion is proved.
			\paragraph{} Consider now $n$ fixed integers satisfying \eqref{eq:2.5.1}. The number of placements of $r$ \textit{distinguishable} balls in $n$ cells resulting in the occupancy numbers $r_1, r_2, \dots, r_n$ is given by \eqref{eq:2.4.6}. Assuming that all $n^r$ possible placements are equally probable, the probability to obtain the given occupancy numbers $r_1,\dots,r_n$ equals
			\begin{equation}
				\label{eq:2.5.4}
				\frac{r!}{r_1!r_2!\cdots r_n!}n^{-r}
			\end{equation}
			This equation \eqref{eq:2.5.4} in physics is known as \textit{\textbf{Maxwell-Boltzmann distribution}}.
			\paragraph{} For the sake of definiteness let us consider the distributions with \textit{occupancy numbers appearing in an arbitrary order}. For an example, we place $r=7$ balls in $n=7$ cells. The occupancy numbers are $2,2,1,1,1,0,0$ in arbitrary order. These seven occupancy numbers induce a partition of the seven cells into three subpopulations consisting, respectively, of the two doubly occupied, the three singly occupied, and the two empty cells. Such a partition into three groups of size 2,3, and 2 can be effected in $\frac{7!}{2!3!2!}$ ways. To each particular assignment of our occupancy numbers to the seven cells there correspond $\frac{7!}{2!2!1!1!1!0!0!}$ different distributions of $r=7$ balls into the seven cells. Accordingly, the total number of distributions such that the occupancy numbers coincide with $2,2,1,1,1,0,0$ in some order is \begin{equation}
				\label{eq:2.5.5}
				\frac{7!}{2!3!2!}\times\frac{7!}{2!2!}
			\end{equation} This is derived by a double application of \eqref{eq:2.4.6}, namely to balls and to cells. More details are on P40.
			\paragraph{}Before continuing, a problem comes out: what's the probabilty of placing 6 balls into 4 cells with no cell empty.
			\begin{enumerate}
			\item The balls are indistinguishable. \\ If the balls are indistinguishable, we can apply \eqref{eq:2.5.3} and \eqref{eq:2.5.4} to get the answer. $$p=\frac{{5\choose 3}}{{9\choose 6}}$$
			\item The balls are distinguishable. \\ At first, we put forward a \textit{wrong} idea. Since the balls are distinguishable, there are $4^6$ ways to place the balls. Then we choose 4 balls from 6 and place them in each cell. The remaining 2 balls have $4^2$ ways to be placed. So the probability is
			$$p=\frac{(6)_4\times 4^2}{4^6}$$
			The reason why this is wrong is that we consider the balls in a specific cell is also ordered (which $|1|2,3|4|$ is different from $|1|3,2|4|$). So we have repeatedly counted it. \\ The correct idea is like the derivation of \eqref{eq:2.5.5}. There are only two assignments of occupancy numbers which are $2,2,1,1$ and $3,1,1,1$ in arbitrary orders where no empty cells. So the probability should be $$p=\frac{\frac{4!}{2!2!}\times\frac{6!}{2!2!1!1!}+\frac{4!}{3!1!}\times\frac{6!}{3!1!1!1!}}{4^6}$$
			\end{enumerate}
			\subsubsection{Maxwell-Boltzmann, Bose-Einstein and Fermi-Dirac Statistics}
			\begin{itemize}
			\item \textbf{Maxwell-Boltzmann Statistics} \\ ?
			\item \textbf{Bose-Einstein Statistics} \\ ?
			\item \textbf{Fermi-Dirac Statistics} \\ ?
			\end{itemize}
			\subsubsection{Application to Runs}
			\paragraph{Run} In any ordered sequence of elements of two kinds, each maximal subsequence of elements of like kind is called a \textit{run}. 
			\paragraph{} For example, the sequence $\alpha\alpha\alpha\beta\alpha\alpha\beta\beta\beta\alpha$ opens with an $\alpha$ run of length 3; it is followed by runs of length 1,2,3,1, respectively. The $\alpha$ and $\beta$ runs alternate so that the total number of runs is always one plus the number of conjunctions of unlike neighbors in the given sequence.
			\paragraph{} The theory of runs is applied in statistics in many ways, but its principal uses are connected with \textit{tests of randomness} or \textit{tests of homogeneity}. The former one is to compute the probability of an observation to determine the observation is intentional or accidental. The latter one focuses on two sequences. These two sequences are similar initially then they are applied different operations. Finally we combine two sequences together to find out the difference between two operations by computing the probability (Please read P42 in details).
		\subsection{The Hypergeometric Distribution}
			\paragraph{} Many combinatorial problems can be reduced to the following form. In a population of $n$ elements $n_1$ are red and $n_2=n-n_1$ are black. A group of $r$ elements is chosen at random. We seek the probability $q_k$ that the group so chosen will contain exactly $k$ red elements. Here $k$ can be any integer between zero and $n_1$ or $r$, whichever is smaller.
			\paragraph{} To find $q_k$, we note that the chosen group contains $k$ red and $r-k$ black elements. The red ones can be chosen in ${n_1 \choose k}$ different ways and the black ones in ${n-n_1 \choose r-k}$. Since any choice of $k$ red elements may be combined with any choice of black ones, we find
			\begin{equation}
				\label{eq:2.6.1}
				q_k = \frac{{n_1\choose k}{n-n_1 \choose r-k}}{{n \choose r}}
			\end{equation}
			The system of probabilities so defined is called the \textbf{\textit{hypergeometric distribution}}. Using \eqref{eq:2.4.3}, it's possible to rewrite \eqref{eq:2.6.1} in the form
			\begin{equation}
				\label{eq:2.6.2}
				q_k = \frac{{r \choose k}{n-r \choose n_1-k}}{{n \choose n_1}}
			\end{equation}
			The formula \eqref{eq:2.6.2} can be understood as: The $n$ elements have not been colored, so we choose $n_1$ elements from $n$ to color them red. Then we are sure that $k$ elements from $r$ is red. So, we have to choose $k$ elements from $r$ and choose another $n_1-k$ elements from the $n-n_1$ to color them red.
			\paragraph{} The probabilities $q_k$ are defined only for $k$ not exceeding $r$ or $n_1$, but since ${a\choose b}$ whenever $b>a$, formulas \eqref{eq:2.6.1} and \eqref{eq:2.6.2} give $q_k=0$ is either $k>n_1$ or $k>r$. Accordingly, the definitions \eqref{eq:2.6.1} and \eqref{eq:2.6.2} may be used for all $k\geq0$, provided the relations $q_k=0$ is interpreted as impossibility.
			\paragraph{} The example (c) on P45 introduces some estimation methods. \textit{Point estimation} (\textit{maximum likelihood estimate}) and \textit{interval estimation} are  mentioned. Both of them will contain some unknown parameters (Please read this example carefully on P45). An interesting property of hypergeometric distribution is put forward here. Considering a hypergeometric distribution, the parameters $n_1, r, k$ are already known and the parameter $n$ is unknown. $q_k(n)$ will get the maximum value when $n=\frac{n_1r}{k}$. This can be proved by calculating the ratio
			\begin{equation}
				\label{eq:2.6.3}
				\frac{q_k(n)}{q_k(n-1)}=\frac{(n-n_1)(n-r)}{(n-n_1-r+k)n}=1+\frac{n_1r-nk}{(n-n_1-r+k)n}
			\end{equation}
			So, when $n<\frac{n_1r}{k}$, $\frac{q_k(n)}{q_k(n-1)} > 1$, vice versa. Finally we get the peak of $q_k(n)$ by assigning $n=\frac{n_1r}{k}$. 
			\paragraph{} From the definition of the probabilities $q_k$ it follows that $$q_0+q_1+q_2+\cdots=1$$ Formula \eqref{eq:2.6.2} therefore implies that for any positive integers $n,n_1,r$
			\begin{equation}
				\label{eq:2.6.4}
				{r \choose 0}{n-r\choose n_1} + {r \choose 1}{n-r \choose n_1-1} + \cdots + {r\choose n_1}{n-r\choose 0} = {n\choose n_1}
			\end{equation}
			This identity is frequently useful.
			\paragraph{} The hypergeometric distribution can easily be generalized to the case where the original population of size $n$ contains several classes of elements. For example, let the population contain three classes of sizes $n_1$, $n_2$ and $n-n_1-n_2$, respectively. If a sample of size $r$ is taken, the probability that it contains $k_1$ elements of the first, $k_2$ elements of the second, and $r-k_1-k_2$ elements of the last class is, by analogy with \eqref{eq:2.6.1}, 
			\begin{equation}
				\label{eq:2.6.5}
				q_{k_1k_2} = \frac{{n_1 \choose k_1}{n_2\choose k_2}{n-n_1-n_2\choose r-k_1-k_2}}{{n \choose r}}
			\end{equation}
			It is, of course, necessary that
			$$k_1\leq n_1,\indent k_2\leq n_2, \indent r-k_1-k_2\leq n-n_1-n_2.$$
		\subsection{Examples for Waiting Times}
			\paragraph{} This section considers once more the conceptual experiment of placing balls randomly into $n$ cells. This time, however, we do not fix in advance the number $r$ of balls but let the balls be placed one by one as long as necessary for a prescribed situation to arise. Two such possible situations will be discussed explicitly: (i) The random placing of balls continues until for the first time a ball is placed into a cell already occupied. (ii) We fix a cell and continue the procedure of placing balls as long as this cell remains empty.
			\paragraph{} Here we analyze these two models respectively.
			\begin{enumerate}
			\item[(i)] It is convenient to use symbols of the form $(j_1,j_2,\dots,j_r)$ to indicate that the first, second, \dots, $r$th ball are placed in cells number $j_1,j_2,\dots,j_r$ and that the process terminates at the $r$th step. This means that the $j_i$ are integers between 1 and $n$; furthermore, $j_1,\cdots,j_{r-1}$ are all different, but $j_r$ equals one among them. Every arrangement of this type represents a sample point and for $r$ only the values $2,3,\dots,n+1$ are possible. 
			\medskip \\ For a fixed $r$ the aggregate of all sample points $(j_1,\dots,j_r)$ represents the event that the process terminates at the $r$th step. The number $j_1,\cdots,j_{r-1}$ can be chosen in $(n)_{r-1}$ different ways; for $j_r$ we have the choice of the $r-1$ numbers $j_1,\cdots,j_{r-1}$. It follows that \textit{the probability of the process terminating at the $r$th step} is
			\begin{equation}
				\label{eq:2.7.1}
				q_r=\frac{(n)_{r-1}(r-1)}{n^r}
			\end{equation} with $q_1=0$ and $q_2=\frac{1}{n}$. The \textit{probability that the process lasts for more than $r$ steps} is $p_r=1-(q_1+q_2+\cdots+q_r)$ or $p_1=1$ and 
			\begin{equation}
				\label{eq:2.7.2}
				p_r = \frac{(n)_r}{n^r}
			\end{equation}
			as can be seen by simple induction. In particular, $p_{n+1}=0$ and $q_1+\cdots+q_{n+1}=1$. The induction to get $p_r$ is to consider $p_r-p_{r-1}$, while
			\begin{equation}
			\begin{aligned}
				p_r-p_{r-1} &= -q_r \\ &= -\frac{(n)_{r-1}(r-1)}{n^r} \\ &=\frac{(n)_{r-1}}{n^{r-1}}\left(\frac{n-r+1}{n}-1\right) \\ &= \frac{(n)_{r}}{n^{r}}-\frac{(n)_{r-1}}{n^{r-1}}
			\end{aligned}
			\end{equation}
			Alternative way to understand $p_r$ is that $p_r$ indicates no cell will contains more than one ball until the $r$th step. It's equivalent to the process lasts for more than $r$ steps.
			\item[(ii)] This model depends on an infinite sample space. The sequences $(j_1,\dots,j_r)$ are now subjected to the condition that the numbers $j_1,\dots,j_{r-1}$ are different from a prescribed number $a\leq n$ but $j_r=a$. For $j_1,\dots,j_{r-1}$ we have $n-1$ choices each, and for $j_r$ no choice at all. For \textit{the probability that the process terminates at the $r$th step} we get therefore
			\begin{equation}
				\label{eq:2.7.4}
				q_r=\left(\frac{n-1}{n}\right)^{r-1}\frac{1}{n}
			\end{equation}
			Summing this geometric series we find $q_1+q_2+\cdots=1$. For \textit{the probability that the process lasts for more than $r$ steps} we get
			\begin{equation}
				\label{eq:2.7.5}
				p_r=1-(q_1+\cdots+q_r) = \left(1-\frac{1}{n}\right)^r
			\end{equation}
			Similarly, $p_r$ can also be yielded by placing the $r$ balls into the $n-1$ cells.
			\end{enumerate}
			\paragraph{} Here, we have a definition that \textit{the median for the distribution}, which is the value of $r$ for which $p_1+\cdots+p_{r-1}\leq 0.5$ but $p_1+\cdots+p_{r} > 0.5$.
		\subsection{Binomial Coefficients}
			\paragraph{} We have used binomial coefficients ${n \choose r}$ only when $n$ is a positive integer, but it is very convenient to extend their definition. The number $(x)_r$ introduced in equation \eqref{eq:2.2.1}, namely
			\begin{equation}
				\label{eq:2.8.1}
				(x)_r=x(x-1)\cdots(x-r+1)
			\end{equation}
			is well defined for all real $x$ provided only that $r$ is a positive integer. For $r=0$ we put $(x)_0=1$. Then
			\begin{equation}
				\label{eq:2.8.2}
				{x \choose r} = \frac{(x)_r}{r!} = \frac{x(x-1)\cdots(x-r+1)}{r!}
			\end{equation}
			\textit{defines the binomial coefficients for all values of $x$ and all positive integers $r$. For $r=0$ we put, as in \eqref{eq:2.4.4}, ${x\choose 0} = 1$ and $0! =1$. For negative integers $r$ we define}
			\begin{equation}
				\label{eq:2.8.3}
				{x \choose r} = 0, \indent r<0
			\end{equation} \textit{We shall never use the symbol ${x\choose r}$ if $r$ is not an integer.}
			\paragraph{} It's easily verified that with this definition we have, for example,
			\begin{equation}
				\label{eq:2.8.4}
				{-1\choose r} = (-1)^r, \indent {-2\choose r} = (-1)^r(r+1)
			\end{equation}
			\paragraph{} Three important properties will be used in the sequel.
			\begin{enumerate}
			\item For \textit{any positive integer $n$, if either $r>n$ or $r<0$,} 
			\begin{equation}
				{n \choose r} = 0
			\end{equation}
			\item For \textit{any number $x$ and any integer $r$,}
			\begin{equation}
				{x \choose r-1}+{x \choose r} = {x+1 \choose r}
			\end{equation}
			\item For \textit{any number $a$ and all values $-1<t<1$, we have \textbf{Newton's binomial formula}}
			\begin{equation}
				\label{eq:2.8.7}
				(1+t)^a = 1+{a\choose 1}t+{a\choose 2}t^2+{a\choose 3}t^3+\cdots
			\end{equation} If $a$ is a positive integer, all terms to the right containing power higher than $t^a$ vanish automatically and the formula is correct for all $t$. If $a$ is not a positive integer, the right side represents an \textit{infinite} series.
			\medskip \\ Using \eqref{eq:2.8.4}, we see that for $a=-1$ the expansion \eqref{eq:2.8.7} reduces to the \textbf{\textit{geometric series}}
			\begin{equation}
				\label{eq:2.8.8}
				\frac{1}{1+t}=1-t+t^2-t^3+t^4-\cdots
			\end{equation}
			Integrating \eqref{eq:2.8.8}, we obtain another formula which will be useful in the sequel, namely, the \textbf{\textit{Taylor expansion of the natural logarithm}}
			\begin{equation}
				\label{eq:2.8.9}
				\ln(1+t)=t-\frac{1}{2}t^2+\frac{1}{3}t^3-\frac{1}{4}t^4+\cdots
			\end{equation} Two alternative forms for \eqref{eq:2.8.9} are frequently used. Replacing $t$ by $-t$ we get
			\begin{equation}
				\label{eq:2.8.10}
				\ln\frac{1}{1-t} = t + \frac{1}{2}t^2+\frac{1}{3}t^3+\frac{1}{4}t^4+\cdots
			\end{equation}
			Adding \eqref{eq:2.8.9} and \eqref{eq:2.8.10} together we find
			\begin{equation}
				\label{eq:2.8.11}
				\ln\sqrt{\frac{1+t}{1-t}}=t+\frac{1}{3}t^3+\frac{1}{5}t^5+\cdots
			\end{equation}
			All these expansions are valid only for $-1<t<1$.
			\end{enumerate}
			\paragraph{} Section 2.12 contains many useful relations derived from \eqref{eq:2.8.7}. Here we mention only that when $a=n$ is an integer and $t=1$, then \eqref{eq:2.8.7} reduces to 
			\begin{equation}
			\label{eq:2.8.12}
			{n\choose 0} + {n\choose 1} + {n \choose 2} + \cdots + {n\choose n} = 2^n
			\end{equation}
			A similar argument shows that the multinomial coefficients \eqref{eq:2.4.6} add to $k^n$.
		\subsection{Stirling's Formula}
			\paragraph{\textit{Stirling's formula:}}  
			\begin{equation}
				\label{eq:2.9.1}
				n!\sim\sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}
			\end{equation}
			\textit{where the sign $\sim$ is used to indicate that the ratio of the two sides tends to unity as $n\rightarrow \infty$}.
			\paragraph{Proof} Our first problem is to derive some sort of estimate for
			\begin{equation}
				\ln n! = \ln 1+\ln 2 + \cdots + \ln n
			\end{equation}
			Since $\ln x$ is a monotone function of $x$ we have
			\begin{equation}
				\int_{k-1}^{k}\ln xdx < \ln k < \int_{k}^{k+1}\ln xdx
			\end{equation}
			Summing over $k=1,\dots,n$ we get
			\begin{equation}
				\int_{0}^{n}\ln xdx < \ln n! < \int_{1}^{n+1}\ln xdx
			\end{equation}
			or
			\begin{equation}
				n\ln n-n < \ln n! < (n+1)\ln(n+1)-n
			\end{equation}
			This double inequality suggests comparing $\ln n!$ with some quantity close to the arithmetic mean of the extreme members. The simplest such quantity is $(n+\frac{1}{2})\ln n - n$, and accordingly we proceed to estimate the difference
			\begin{equation}
				\label{eq:2.9.6}
				d_n=\ln n! - (n+\frac{1}{2})\ln n - n
			\end{equation}
			Note that
			\begin{equation}
				d_n-d_{n+1}=(n+\frac{1}{2})\ln \frac{n+1}{n}-1
			\end{equation}
			But
			\begin{equation}
				\frac{n+1}{n}=\frac{1+\frac{1}{2n+1}}{1-\frac{1}{2n+1}}
			\end{equation}
			and using the expansion \eqref{eq:2.8.11} we get
			\begin{equation}
				\label{eq:2.9.9}
				d_n-d_{n+1}=\frac{1}{3(2n+1)^2}+\frac{1}{5(2n+1)^4}+\cdots
			\end{equation}
			By comparison of the right side with a geometric series with ratio $(2n+1)^{-2}$ one sees that
			\begin{equation}
				\label{eq:2.9.10}
				0 < d_n-d_{n+1} < \frac{1}{3[(2n+1)^2-1]}=\frac{1}{12n}-\frac{1}{12(n+1)}
			\end{equation}
			From \eqref{eq:2.9.9} we conclude that the sequence $\{d_n\}$ is decreasing, while \eqref{eq:2.9.10} shows that the sequence $\{d_n-\frac{1}{12n}\}$ is increasing. It follows that finite limit
			\begin{equation}
				C = \lim d_n
			\end{equation}
			exists. But in view of \eqref{eq:2.9.6} the relation $d_n\rightarrow C$ is equivalent to
			\begin{equation}
				n! \sim e^Cn^{n+\frac{1}{2}}e^{-n}
			\end{equation}
			This is the Stirling's formula, and the constant $C$ will be proved later that $e^C = \sqrt{2\pi}$. The value of $e^C$ is connected with the normal approximation thearem which will be mentioned in chapter VII.
			\paragraph{Refinements} The inequality \eqref{eq:2.9.10} has a companion inequality in the reverse direction. Indeed, from \eqref{eq:2.9.9} it is obvious that
			\begin{equation}
				d_n-d_{n+1} > \frac{1}{3(2n+1)^2} > \frac{1}{12n+1} - \frac{1}{12(n+1)+1}
			\end{equation} It follows that the sequence $\{d_n-\frac{1}{12n+1}\}$ decrease. Since $\{d_n-\frac{1}{12n}\}$ increases this implies the double inequality
			\begin{equation}
				C+\frac{1}{12n+1} < d_n < C+\frac{1}{12n}
			\end{equation}
			Substituting into \eqref{eq:2.9.6}, and anticipating that $e^C=\sqrt{2\pi}$, we get
			\begin{equation}
				\label{eq:2.9.15}
				\sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}e^{\frac{1}{12n+1}} < n! < \sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}e^{\frac{1}{12n}}
			\end{equation}
			This double inequality supplement Stirling's formula in a remarkable manner. The right-hand member in \eqref{eq:2.9.15} overestimates $n!$, but with an error of less than $0.8n^{-2}$ percent. After doing some experiments, the both part of the \eqref{eq:2.9.15} estimate better than raw formula in \eqref{eq:2.9.1}, and the right-hand member of \eqref{eq:2.9.15} have a smaller error than the left-hand member.
		\subsection{Problems for Solution}
		\subsubsection{Exercises and Examples}
		\begin{enumerate}
		\item (a) $ans=26^3$ \\ (b) $ans=26^3+26^2$ \\ (c) $ans=26^4+26^3+26^2$
		\item $ans=2^{10}+2^9+\cdots+2=2^{11}-2$
		\item $ans = \frac{n^2}{2}$
		\item (a) $p=\frac{(n-1)2!(n-2)!}{n!} = \frac{2}{n}$ \\ (b) $p=\frac{(n-2)3!(n-3)!}{n!} = \frac{6}{n(n-1)}$
		\item $p_a = 1-\frac{5^6}{6^6}$ and $p_b = 1-\frac{5^{12}+12\times5^{11}}{6^{12}}$, \\so $p_a-p_b = \left(\frac{17}{5}\times\frac{5^{6}}{6^{6}}-1\right)\frac{5^6}{6^6} < 0 $, \\ \textit{A} has the greater probability to win.
		\item (a) $p_{a1}=\frac{10}{10^3} = 0.01$, $p_{a2} = \frac{{10\choose 2}\frac{2!}{1!1!}\frac{3!}{2!1!}}{10^3} = 0.27$, $p_{a3} = \frac{(10)_3}{10^3} = 0.72$\\ (b) $p_{b1} = \frac{10}{10^4} = 0.001$, $p_{b2}=\frac{{10\choose 2}\left(\frac{2!}{1!1!}\frac{4!}{3!1!}+\frac{2!}{2!}\frac{4!}{2!2!}\right)}{10^4} = 0.063$, \\  $p_{b3}=\frac{{10\choose 3}\frac{3!}{2!1!}\frac{4!}{2!1!1!}}{10^4} = 0.432 $, $p_{b4} = \frac{(10)_4}{10^4} = 0.504$
		\item $p_r = \frac{(10)_r}{10^r}$, $p_{10} = \frac{10!}{10^{10}} \approx \frac{\sqrt{2\pi}10^{10+0.5}e^{-10}}{10^{10}} = 0.00035987$
		\item $p_a=\frac{9^k}{10^k}$, $p_b=\frac{9^k}{10^k}$, $p_c=\frac{8^k}{10^k}$, $p_d= \frac{9^k+9^k-8^k}{10^k}$
		\item (a) If the balls are indistinguishable, then $$p_a=\frac{{n \choose 1}{n-1 \choose n-2}}{{2n-1\choose n}}$$ \\ (b) If the balls are distinguishable, then $$p_b=\frac{{n\choose 1}\frac{(n-1)!}{1!(n-2)!}\frac{n!}{2!1!1!\cdots1!}}{n^n}$$
		\item The probability of the obervation is $p = \frac{9}{{12\choose 4}} = 0.018$. So, it seems that this arragement is indicative of non-randomness.
		\item The probability of using $k (k=1,2,\cdots,n)$ trials to open the door is $p=\frac{(n-1)_{k-1}}{(n)_k}=\frac{1}{n}$.
		\item (a) $p_a = \frac{1}{(2n)!}$  (b) $p_b = \frac{n!n!}{(2n)!}$
		\item $p = \frac{2^{12}}{7^{12}} = 2.96\times10^{-7}$, so his renting a garage only for Tue and Thu was justified.
		\item $p = \frac{6^{12}}{7^{12}} = 0.157$, so this can't prove that no tickets are given on Sun.
		\item $p = \frac{{90 \choose 10}}{{100 \choose 10}} = 0.330$
		\item $p = 1-\frac{5\times4^{25}}{5^{25}} = 0.981$
		\item $p_a = \frac{2(n-r-1)(n-2)!}{n!} = \frac{2(n-r-1)}{n(n-1)}$ \\ $p_b=\frac{(n-2)!}{(n-1)!} = \frac{1}{n-1}$
		\item (a) If dice are distinguishable, $p_a = \frac{6^3}{6^3\times6^3} = \frac{1}{216}$. \\ (b) If dice are indistinguishable, $p_b = \frac{{8\choose 3}}{{8 \choose 3}\times{8 \choose 3}} = \frac{1}{56}$.
		\item $p_a = 1-\frac{5^4}{6^4} = 0.518$, $p_b = 1-\left(1-\frac{1}{36}\right)^{24} = 0.491$. So, the assert is proved.
		\item (a) Without replacement, $p_a = \frac{(n-N)_r}{(n)_r}$. \\ (b) With replacement, $p_b = \frac{(n-N)^r}{n^r}$. \\ (i) When $n=100, r=N=3$, $p_a = 0.912$, $p_b = 0.913$ \\ (ii) When $n=100, r=N=10$, $p_a = 0.330$, $p_b = 0.737$
		\item (a) $p_a = \frac{n(n-1)^{r-1}}{n^r} = \frac{(n-1)^{r-1}}{n^{r-1}} = \left(\frac{n-1}{n}\right)^{r-1}$ \\ (b) $p_b = \frac{(n)_r}{n^r}$ \\ ?(c) $p_c = \frac{{n\choose N}{n-1\choose N}^{N(r-1)}}{{n\choose N}{n\choose N}^{N(r-1)}} = \frac{{n-1\choose N}^{N(r-1)}}{{n\choose N}^{N(r-1)}} = \left(\frac{n-N}{n}\right)^{N(r-1)}$ \\ ?(d) $p_d = \frac{{n\choose N}{n-N \choose N}{n-2N\choose N}\cdots{n-N(r-1)N \choose N}}{{n\choose N}{n\choose N}^{N(r-1)}} = \frac{\prod_{k=1}^{N(r-1)}{n-kN\choose N}}{{n\choose N}^{N(r-1)}}$, \\ here we let $A=N(r-1)$ and we can reduce $p_d$ into $p_d = \frac{{n-N\choose AN}\frac{(AN)!}{(N!)^A}}{{n\choose N}^A}$, note that $\frac{(AN)!}{(N!)^A}$ is a multinomial coefficient, denoting another way to understand this problem.
		\item ? $p_r = \frac{{n\choose 2}{n-1\choose2}^{2(r-1)}}{{n\choose 2}{n\choose 2}^{2(r-1)}} = \left(\frac{n-2}{n}\right)^{2(r-1)}$. \\Supposing $n$ is large enough, the median $r_{mid}$ is the least value of $r$ satisfying $\sum_{i=1}^{r}p_i > 0.5$. While $$\lim_{n\rightarrow\infty}\sum_{i=1}^{r}p_i=\lim_{n\rightarrow\infty}\frac{1-\left(\frac{n-2}{n}\right)^{2r}}{1-\left(\frac{n-2}{n}\right)^2}=$$
		\item If the girls have same probabilities to break a dish, the probability that three of four breakages are caused by one girl is $p = \frac{{4\choose 3}{3\choose 1}}{4^4} = 0.047$. The probability is small and we can regard the youngest girl is clumsy.
		\item (a) $p_a = \frac{12!}{12^{12}} = 5.37\times10^{-5}$. (b) $p_b = \frac{{12\choose 2}(2^6-2)}{12^6} = 1.37\times10^{-3}.$
		\item $p = \frac{\frac{12!}{6!6!}\frac{30!}{(3!)^6(2!)^6}}{12^{30}} = 3.45\times10^{-4}$
		\item (a) $p_a = \frac{{n\choose 2r}{2\choose 1}}{{2n\choose 2r}}$ (b) $p_b = \frac{{n\choose 1}{n-1\choose 2r-2}{2\choose 1}}{{2n\choose 2r}}$ (c) $p_c = \frac{{n\choose 2}{n-2\choose 2r-4}{2\choose 1}}{{2n\choose 2r}}$
		\item $p = \frac{{N-3 \choose N-r-2}}{{N-1\choose N-r}}$
		\item $p = \frac{{2N\choose N}{2N\choose N}}{{4N \choose 2N}} = \frac{((2N)!)^4}{(N!)^4(4N)!} \approx \sqrt{\frac{2}{\pi N}}$
		\item $p_a = \frac{{4\choose k}{48 \choose 13-k}}{{52 \choose 13}}$, $p_b = \frac{{4\choose k}{48\choose 13-k}\frac{39!}{(13!)^3}}{\frac{52!}{(13!)^4}} = \frac{{4\choose k}{48\choose 13-k}}{\frac{52!}{39!13!}} = p_a$. Proved.
		\item $p_a = \frac{{13\choose m}{39\choose 13-m}{26+m\choose 13 -n}}{{52\choose 13}{39\choose 13}}$, $p_b = \frac{{13\choose m}{39\choose 13-m}{26+m\choose 13-n}\frac{26!}{13!13!}}{\frac{52!}{(13!)^4}} = \frac{{13\choose m}{39\choose 13-m}{26+m\choose 13-n}}{\frac{52!}{13!39!}\frac{39!}{13!26!}} = p_a $. Proved.
		\item 
		\end{enumerate}
		\subsubsection{}
		\subsubsection{}
	\newpage
	\section{Fluctuations in Coin Tossing and Random Walks}
			
	\newpage
	\section{Combination of Events}
					
	\newpage
	\section{Conditional Probability, Stochastic Independence}
			
		
\end{document}
